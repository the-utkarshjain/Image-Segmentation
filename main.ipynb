{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA3 CNN for Semantic Segmentation\n",
    "\n",
    "Deliverables:\n",
    "- Avg pixel accuracy. (Boundary is exclusive)\n",
    "- Avg IoU. (Boundary is exclusive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Get and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python download.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from util import *\n",
    "from voc import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import *\n",
    "\n",
    "'''\n",
    "Image shape : (224, 224, 3)\n",
    "Num samples : train 209, val 213, test 210\n",
    "Num classes : 21\n",
    "Images are in range (0,1)\n",
    "Masks have values {0,1}\n",
    "'''\n",
    "loader = DataLoader(VOC('train') , batch_size=5, shuffle=False)\n",
    "images, masks = next(iter(loader)) # (B, 3, H, W), (B, H, W)\n",
    "images = images.detach().numpy()\n",
    "masks = masks.detach().numpy()\n",
    "\n",
    "anns = []\n",
    "for mask in masks:\n",
    "    classes = []\n",
    "    for label in np.unique(mask):\n",
    "        classes.append(f'{label}: ' + class_dict()[label]) \n",
    "    anns.append(\"\\n\".join(classes))\n",
    "plot_images(images)\n",
    "plot_images(masks, pallet=voc_pallet(), annotations=anns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Implement Evaluation Metric: IoU and Pixel acc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "from voc import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import *\n",
    "\n",
    "dataset = VOC('train')\n",
    "img, mask = dataset.__getitem__(0)\n",
    "\n",
    "iou = compute_iou(np.zeros((224,224)), mask)\n",
    "acc = compute_pixel_acc(np.zeros((224,224)), mask)\n",
    "print(f'IoU: {iou}, Acc: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Baseline Model\n",
    "- Optim: Adam / Adamw\n",
    "- Use early stoppling\n",
    "- Desired result: 0.65 acc, 0.005 IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "from voc import *\n",
    "from model import *\n",
    "from train import *\n",
    "\n",
    "import torchvision.transforms\n",
    "\n",
    "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = {\n",
    "    'epochs'    : 20,\n",
    "    'bz'        : 16,\n",
    "    'lr'        : 5e-4,\n",
    "    'device'    : device,\n",
    "    'early_stop': 5,\n",
    "    'remark'    : 'baseFCN'\n",
    "}\n",
    "\n",
    "# Transformations\n",
    "mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "input_transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(*mean_std)\n",
    "    ])\n",
    "target_transform = MaskToTensor()\n",
    "TF_transform = None\n",
    "\n",
    "# Dataset and Dataloader initialization\n",
    "train_loader, val_loader, test_loader = get_train_val_test_loader(\n",
    "    input_transform, target_transform, TF_transform, config['device'], config['bz'])\n",
    "\n",
    "''' Prepare model '''\n",
    "fcn_model = FCN_baseline(n_class=21)\n",
    "fcn_model.apply(init_weights)\n",
    "fcn_model = fcn_model.to(config['device'])\n",
    "\n",
    "optimizer = torch.optim.Adam(fcn_model.parameters(), config['lr'])\n",
    "criterion =  torch.nn.CrossEntropyLoss()\n",
    "\n",
    "''' Train model '''\n",
    "best_iou_score, best_accuracy, min_validation_loss, \\\n",
    "training_loss_history, validation_loss_history, early_stop_epoch \\\n",
    "    = train(fcn_model, train_loader, val_loader, criterion, optimizer, config)\n",
    "\n",
    "''' Test model '''\n",
    "_, iou, acc = val(fcn_model, test_loader, criterion)\n",
    "print(f'Test IOU: {round(iou, 3)}. Test acc: {round(acc, 3)}')\n",
    "\n",
    "''' Visualize some test sample '''\n",
    "imgs, masks_gt = next(iter(test_loader))\n",
    "imgs, masks_gt = imgs[:5], masks_gt[:5]\n",
    "masks_gt = F.one_hot(masks_gt.to(torch.int64), num_classes=21).permute(0, 3, 1, 2).to(torch.float64)\n",
    "masks_pred = fcn_model(imgs)\n",
    "masks_pred = torch.argmax(masks_pred, dim=1).cpu().numpy()\n",
    "masks_gt = torch.argmax(masks_gt, dim=1).cpu().numpy()\n",
    "plot_images(np.concatenate([masks_gt,masks_pred]), voc_pallet())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Improve Baseline Model\n",
    "- LR schedule: cosine annealing\n",
    "- Augment dataset: flip, rotate, crop\n",
    "- Address class imbalance issue with modified loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "from voc import *\n",
    "from model import *\n",
    "from train import *\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = {\n",
    "    'epochs'    : 20,\n",
    "    'bz'        : 16,\n",
    "    'lr'        : 0.001,\n",
    "    'device'    : device,\n",
    "    'early_stop': 5,\n",
    "    'remark'    : 'modifiedFCN_scheduler'\n",
    "}\n",
    "\n",
    "''' Transformations '''\n",
    "mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "input_transform = standard_transforms.Compose([\n",
    "        standard_transforms.ToTensor(),\n",
    "        standard_transforms.Normalize(*mean_std)\n",
    "    ])\n",
    "target_transform = MaskToTensor()\n",
    "TF_transform = lambda x: [x, TF.hflip(x), TF.rotate(x.unsqueeze(0), angle = 5, fill = 0).squeeze(0), TF.rotate(x.unsqueeze(0), angle = -5, fill = 0).squeeze(0), *TF.five_crop(x, (224, 224))]\n",
    "\n",
    "''' Data loaders '''\n",
    "train_loader, val_loader, test_loader = get_train_val_test_loader(\n",
    "    input_transform, target_transform, TF_transform, config['device'], config['bz'], collate_fn=collate_fn)\n",
    "\n",
    "# ''' Prepare model '''\n",
    "fcn_model = FCN_baseline()\n",
    "fcn_model.apply(init_weights)\n",
    "fcn_model = fcn_model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(fcn_model.parameters(), config['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, config['epochs'], eta_min = 0.0001)\n",
    "\n",
    "class_weight = getClassWeights(input_transform, target_transform, TF_transform)\n",
    "criterion =  torch.nn.CrossEntropyLoss(weight = class_weight)\n",
    "\n",
    "''' Train model '''\n",
    "best_iou_score, best_accuracy, min_validation_loss, \\\n",
    "training_loss_history, validation_loss_history, early_stop_epoch \\\n",
    "    = train(fcn_model, train_loader, val_loader, criterion, optimizer, scheduler, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Test model '''\n",
    "test_loss, iou, acc = modelTest(fcn_model, test_loader, criterion, early_stop_epoch)\n",
    "print(f'Test Loss: {test_loss}, Test IOU: {iou}. Test acc: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(training_loss_history, validation_loss_history, \"./Figures/part4d_train_val_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, masks_gt = next(iter(test_loader))\n",
    "imgs, masks_gt = next(iter(test_loader))\n",
    "imgs, masks_gt = imgs[36:37], masks_gt[36:37]\n",
    "print(imgs.shape, masks_gt.shape)\n",
    "masks_gt = F.one_hot(masks_gt.to(torch.int64), num_classes=21).permute(0, 3, 1, 2).to(torch.float64)\n",
    "masks_pred = fcn_model(imgs)\n",
    "masks_pred = torch.argmax(masks_pred, dim=1).cpu().numpy()\n",
    "masks_gt = torch.argmax(masks_gt, dim=1).cpu().numpy()\n",
    "plot_images(np.concatenate([masks_gt,masks_pred]), voc_pallet(), cols=2, path='./Figures/part4d_test.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "c17ef166827587e58f5e1d9fc37d7d71128d9ee001cf728f6b987f1c2dd16782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
